---
title: "TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition"
collection: publications
category: manuscripts
permalink: /publication/2025-05-22-tat-vpr-ternary-adaptive-transformer-for-dynamic-and-efficient-visual-place-recognition
excerpt: 'TAT-VPR fuses ternary weight quantization with a learned activation-sparsity gate, giving visual SLAM systems a 5 × smaller model and up to 40 % fewer operations while retaining state-of-the-art Recall@1.'
date: 2025-05-22
venue: 'arXiv preprint'
slidesurl: ''
paperurl: 'https://arxiv.org/abs/2505.16447'
citation: 'Grainge, O., Milford, M., Bodala, I., Ramchurn, S. D., &amp; Ehsan, S. (2025). &quot;TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition.&quot; <i>arXiv preprint</i>, arXiv:2505.16447.'
---

## What’s inside
* **Dynamic accuracy–efficiency trade-offs.** A top-k activation mask lets you dial computation down by up to 40 % at run time with < 1 % loss in Recall@1. :contentReference[oaicite:0]{index=0}
* **Extreme compression.** Ternary weights (2-bit) shrink the backbone five-fold, freeing memory on micro-UAV and embedded SLAM stacks.
* **Distillation keeps performance high.** A two-stage token-level distillation from a full-precision DINOv2-BoQ teacher preserves descriptor quality despite aggressive quantization. :contentReference[oaicite:1]{index=1}

## Key benchmarks
* **Pitts30k:** Near-dense Recall@1 with up to 40 % fewer TOPs.
* **SVOX condition splits:** Outperforms convolutional baselines under snow, rain, and night conditions while using an order of magnitude less memory.

## TL;DR
TAT-VPR shows that extreme quantization and adaptive sparsity aren’t mutually exclusive—you can have a single model that scales from high-accuracy desktop runs down to ultra-efficient on-board inference without retraining.
